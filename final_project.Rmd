---
title: "606 Final Project"
author: "Alec"
date: "12/2/2021"
output: 
  prettydoc::html_pretty:
    theme: cosmo
    highlight: github
    keep_md: true
    toc: true
    toc_depth: 2
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE)
```

# Load Packages

```{r results='hide', message=FALSE, warning=FALSE}
library(tidyverse)
library(psych)
library(stringr)
library(ggmap)
library(readr)
```

# Load and Process Data

## Original Post Office Data

Original dataset, post first-round geocoding, contains all 249 post offices in NYC (5 Boroughs).

```{r results='hide', message=FALSE, warning=FALSE}
location_data <- read_csv("data/post_office_coords.csv", show_col_types = FALSE)
```

## Manufactured Routes Dataset

After randomizing the rows of the above address dataset, I created every combination of 2 addresses without repetition. This requires the assumption that the travel duration between any two points in the city is the same in both directions (which is likely not the case). The reason for this decision is due to financial limitations of google maps API (gmaps), which was used to compute the travel distance and duration for each combinatin of addresses. gmaps provides users with a generous amount of free queries after signup, but as it is my personal account I did not want to cross the free-threshold.

In total there are 30,876 route combinations used in the subsequent analysis.

```{r results='hide', message=FALSE, warning=FALSE}
data <- read_csv("data/route_data.csv", show_col_types = FALSE)
```

## Data Cleaning and Additional Feature Engineering

There are instances where different post offices operated in separate units of the same building. In these situations the geodesic distance calculated with geopy and the google_duration provided by gmaps would equal 0, which should not be included in the model.

```{r}
data <- data %>%
  filter(
    google_duration > 300
  )
```

In addition to the is_same_borough feature which was created using python, here I create a function that will extract an address' zip code, later to be used to create is_same_zip feature for a route

```{r}
extract_zip <- function(address) {
  zip <- str_extract(address, "\\d{5}")
  
  return(zip)
}
```

Additionally, since we have the data, we should test to see if adding a categorical value designating which two boroughs were involved in a given route will provide value to a model.

```{r}
get_borough_path <- function(path_string) {
  split <- unlist(str_split(path_string,","))
  b1 <- split[1]
  b2 <- split[2]
  b_vector <- c(b1, b2)
  sorted <- sort(b_vector)
  return(str_c(sorted, collapse=","))
}
```


```{r}
borough_paths <- lapply(str_c(data$origin_borough, data$destination_borough, sep=","), get_borough_path)
```


```{r}
data <- data %>%
  mutate(
    origin_zip = extract_zip(origin_address),
    destination_zip = extract_zip(destination_address),
    is_same_zip = origin_zip == destination_zip,
    borough_path = unlist(borough_paths),
    log_norm_geodesic = scale(log(geodesic_distance),center=TRUE, scale=TRUE)
  ) %>%
  select(origin_address, 
         origin_zip,
         origin_borough, 
         origin_coordinate,
         destination_address,
         destination_zip,
         destination_borough,
         destination_coordinate,
         is_same_zip,
         is_same_borough,
         borough_path,
         geodesic_distance,
         log_norm_geodesic,
         google_distance,
         google_duration)
```

# EDA and Data Visualization

## Address Dataset

The routes dataset is based on the initial address dataset, which includes 249 post office addresses in teh 5 boroughs of NYC.

```{r}
location_data %>%
  ggplot() +
  geom_bar(aes(x=Borough, fill=Borough)) +
  theme(legend.position = "none")
```

### Generating map plot

Below we create two functions that extract the latitude and longitude from the initial address dataset (Coordinate was saved as a string).

```{r}
extract_lat <- function(coordinate_string) {
  lat <- unlist(str_split(str_replace_all(coordinate_string,"\\(?\\)?",""),","))[1]
  
  return(lat)
}
```

```{r}
extract_long <- function(coordinate_string) {
  long <- unlist(str_split(str_replace_all(coordinate_string,"\\(?\\)?",""),","))[2]
  
  return(long)
}
```

```{r}
lat_data = lapply(location_data$Coordinate, extract_lat)
long_data = lapply(location_data$Coordinate, extract_long)
```

Next we create another dataframe in the correct format for gmaps

```{r}
coord_data <- location_data %>%
  select(
    Address,
    Coordinate
  ) %>%
  mutate(
    lat = as.numeric(unlist(lat_data)),
    long = as.numeric(unlist(long_data))
  ) %>%
  select(
    Address,
    lat,
    long
  )
```


```{r results=FALSE, message=FALSE}
api_key <- read_csv("key_file.csv", show_col_types = FALSE)$key[1]
```

```{r}
register_google(key = api_key)
```


```{r}
nymap <- get_map(location = c(lon = mean(coord_data$long), lat = mean(coord_data$lat)), zoom = 10,
                      maptype = "satellite", scale = 2)
```

```{r}
ggmap(nymap) +
  geom_point(data = coord_data, aes(x = long, y = lat, fill = "red", alpha = 0.8), size = 2, shape = 21) +
  guides(fill=FALSE, alpha=FALSE, size=FALSE)
```


## Routes Dataset

```{r}
describe(data$google_duration)
```
From the below histogram the response variable 'google_duration' (how long google estimates a trip between two points will tage) is normally distributed with a mean of 1897.7 and a standard deviation of 727.13

```{r}
data %>%
  ggplot() +
  geom_histogram((aes(x=google_duration)))
```

```{r}
describe(data$geodesic_distance)
```
Unlike 'google_duration', the primary independent variable 'geodesic_distance' has a clear right skew and potentially breaches assumption of normality.

The Shapiro test confirms non-normality with a p-value close to zero.


```{r}
shapiro.test(sample(data$geodesic_distance,100))
```

```{r}
data %>%
  ggplot() +
  geom_histogram((aes(x=geodesic_distance)))
```

```{r}
qqnorm(data$geodesic_distance)
qqline(data$geodesic_distance)
```
Let's test if the log is any better

```{r}
data %>%
  ggplot() +
  geom_histogram((aes(x=log_norm_geodesic)))
```

It seems that the log is not much better than standard geodesic_distance.



```{r}
qqnorm(data$log_norm_geodesic)
qqline(data$log_norm_geodesic)
```
```{r}
shapiro.test(sample(data$log_norm_geodesic,100))
```

```{r}
cor(data$geodesic_distance, data$google_duration)
```


```{r}
data %>%
  ggplot(aes(x=geodesic_distance, y=google_duration)) +
  geom_point() +
  geom_smooth(method="lm")
```



Let's see if we can see a significant effect on the response based on different 'borough_paths'

```{r}
data %>%
  ggplot() +
  geom_point(aes(x=geodesic_distance, y=google_duration)) +
  facet_wrap(~borough_path)
```

```{r}
data %>%
  ggplot() + 
  geom_boxplot(aes(x=google_duration, y=borough_path), fill="purple")
```

# Perform ANOVA test
```{r}
anova <- aov(google_duration ~ borough_path, data = data)

summary(anova)
```

```{r}
# Following this F Test, we confirm that the 'borough_path' has a significant impact on the ultimate google_duration and that this variable should be retaiend in our model.
```


# Create Linear Model

## Baseline model using just geodesic distance

```{r}
baseline = lm(google_duration ~ geodesic_distance, data=data)
```

```{r}
summary(baseline)
```

Interpreting the above, we can write out the following linear model:

google_duration = 772.8249 + 73.1228*geodesic_distance

Intercept: assuming geodesic_distance of 0, the google_duration will be roughly 773 seconds

Slope: for each additinal km added to geodesic distance, we can expect that google_duration will increase be roughly 73 seconds.

## Baseline log


```{r}
log_model = lm(google_duration ~ log_norm_geodesic, data=data)
```

```{r}
summary(log_model)
```


## Adding additional features

```{r}
feature_model = lm(google_duration ~ poly(geodesic_distance,1) + is_same_zip + borough_path, data=data)
```

```{r}
summary(feature_model)
```

The Adjusted R Squared went up a lot!

## Testing out polynomials

```{r}
poly_model = lm(google_duration ~ poly(geodesic_distance,2) + is_same_zip  + borough_path, data=data)
```

```{r}
summary(poly_model)
```

I am happy with this model. Let's see what the RSME is!

```{r}
sqrt(sum(poly_model$residuals^2) / length(poly_model$residuals))
```

Not bad at all! RSME is 294 seconds, or just under 5 minutes.

294/60



```{r}
ggplot(data = poly_model, aes(x = .fitted, y = .resid, color=borough_path, alpha=.1)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```
```{r}
poly_model %>%
  ggplot() +
  geom_boxplot(aes(x=.resid, y=borough_path), fill="purple") +
  xlab("Residuals") +
  ylab("Borough Path")
```

```{r}
qqnorm(poly_model$residuals)
qqline(poly_model$residuals)
```

```{r}
ggplot() +
  geom_histogram(aes(x=poly_model$residuals))
```

```{r}
data <- data %>%
  mutate(
    predictions = poly_model$fitted.values,
    residuals = poly_model$residuals
  )
```

```{r}
data %>%
  arrange(residuals)
```


2012 / 60



```{r}
get_address_residual <- function(address){
  in_scope_df <- data%>%
                  filter(
                    origin_address == address | destination_address == address
                  )
  avg_residual = mean(abs(in_scope_df$residuals))
  
  return(avg_residual)
  
}
```


```{r}
coord_data <- coord_data %>%
  mutate(
    average_residual = unlist(lapply(coord_data$Address, get_address_residual)),
    residual_group = ntile(average_residual, 20))
```


```{r}
ggmap(nymap) +
  geom_point(data = coord_data, aes(x = long, y = lat, color = residual_group), size = 2, shape = 20, position="jitter") +
  scale_color_gradient(low="green", high="red")
```


```{r}
ggmap(nymap) +
  geom_point(data = coord_data[coord_data$residual_group %in% c(18,19,20),], aes(x = long, y = lat), color = "orange", size = 2, shape = 20, position="jitter")
```
Over Predictions
14506 243rd St, Rosedale, NY 11422
626 Sheepshead Bay Rd Ste 8, Brooklyn, NY 11224

Under Predictions
45 Bay St Ste 2, Staten Island, NY 10301
1369 Broadway, Brooklyn, NY 11221


```{r}
under_predictions <- coord_data %>%
  mutate(
    type = "under"
  ) %>%
  filter(
    str_detect(Address, "45 Bay St Ste 2") | str_detect(Address, "1369 Broadway")
  )
```

```{r}
over_predictions <- coord_data %>%
  mutate(
    type = "over"
  ) %>%
  filter(
    str_detect(Address, "14506 243rd St") | str_detect(Address, "626 Sheepshead Bay Rd")
  )
```

```{r}
bad_predictions <- rbind(over_predictions, under_predictions)
```


```{r}
ggmap(nymap) +
  geom_point(data = bad_predictions, aes(x = long, y = lat, color = type), size = 4, shape = 20, position="jitter")
```
